{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter note book includes some codes to start WebScraping. We will use a package called \"BEAUTIFUL SOUP\" to collect data from web.\n",
    "Once you've collected your data and stored it in .csv file you can start analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 20\n",
    "page_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n"
     ]
    }
   ],
   "source": [
    "# loading the required pages by getting requests from web\n",
    "\n",
    "for t in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {t}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{t}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response=requests.get(url).content\n",
    "    soup=BeautifulSoup(response,\"lxml\")\n",
    "    review_box = soup.find_all(\"article\",{\"itemprop\":\"review\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists and dict for creating DF\n",
    "\n",
    "date_li=[]\n",
    "name_li=[]\n",
    "rating_li=[]\n",
    "verify_li=[]\n",
    "review_li=[]\n",
    "\n",
    "columns=[ 'Seat Type', 'Recommended','Aircraft', 'Type Of Traveller', 'Route']  \n",
    "        \n",
    "valued_col=['Ground Service', 'Wifi & Connectivity','Seat Comfort','Value For Money', 'Inflight Entertainment',\n",
    "                'Food & Beverages', 'Cabin Staff Service']\n",
    "\n",
    "\n",
    "dict2 = {'Aircraft':[], 'Seat Type':[],'Type Of Traveller':[] ,'Recommended':[] ,'Ground Service':[],\n",
    "        'Wifi & Connectivity':[], 'Seat Comfort':[], \n",
    "        'Value For Money':[], 'Inflight Entertainment':[],\n",
    "        'Food & Beverages':[], 'Cabin Staff Service':[],'Route':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding Unique values in table columns\n",
    "\n",
    "list=[]\n",
    "for rev in review_box:\n",
    "    table=rev.find(\"table\",{\"class\":\"review-ratings\"})\n",
    "    rows = table.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        data=row.find_all(\"td\")\n",
    "        for x in data:\n",
    "            list.append(x.get_text())\n",
    "\n",
    "double_li = [list[x:x+2] for x in range(0,len(list),2)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# to find column elements\n",
    "l=[]\n",
    "for x in double_li:\n",
    "    t=x[0]\n",
    "    l.append(t)     # appending table columns to a list \"l\"\n",
    "s=set(l)            # getting unique values\n",
    "# print(s)\n",
    "# len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in range(1, pages + 1):\n",
    "#     review_box = soup.find_all(\"article\",{\"itemprop\":\"review\"})\n",
    "#     for rev in review_box:\n",
    "#         print(rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting published date\n",
    "\n",
    "for t in range(1, pages + 1):\n",
    "   \n",
    "    for rev in review_box:\n",
    "        date = rev.find(\"meta\")[\"content\"]\n",
    "        if date == None:\n",
    "            date_li.append(\"NA\")\n",
    "        elif date != None:\n",
    "            date_li.append(date)\n",
    "# print(date_li)    \n",
    "# print(len(date_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, pages + 1):  #getting overall rating\n",
    "    for rev in review_box:\n",
    "        rating=rev.find(\"div\",{\"class\":\"rating-10\"}).find(\"span\",{\"itemprop\":\"ratingValue\"})\n",
    "        if rating == None:\n",
    "            rating_li.append(\"NA\")\n",
    "        elif rating !=None:\n",
    "            rating_li.append(rating.text)\n",
    "# print(rating_li)   \n",
    "# print(len(rating_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, pages + 1): # getting name of customer\n",
    "    for rev in review_box:\n",
    "    \n",
    "        names=rev.find(\"h3\").find(\"span\",{\"itemprop\":\"author\"}).find(\"span\",{\"itemprop\":\"name\"})\n",
    "        if names == None:\n",
    "            name_li.append(\"NA\")\n",
    "        elif names != None:\n",
    "            name_li.append(names.get_text())\n",
    "        \n",
    "# print(name_li)\n",
    "# print(len(name_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " # checking and getting verification status of review\n",
    "\n",
    "for t in range(1, pages + 1):   \n",
    "    for rev in review_box:\n",
    "        verify=rev.find(\"div\",{\"class\":\"tc_mobile\"}).find(\"div\",{\"class\":\"text_content\"}).find(\"a\")\n",
    "        if verify==None:\n",
    "            verify_li.append(\"NA\")\n",
    "        elif verify != None:\n",
    "            verify_li.append(verify.get_text())\n",
    "\n",
    "\n",
    "\n",
    "# print(verify_li)\n",
    "# print(len(verify_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting customer review\n",
    "\n",
    "for t in range(1,pages+1):   \n",
    "    for rev in review_box:\n",
    "        review=rev.find(\"h2\")\n",
    "        if review == None:\n",
    "            review_li.append(\"NA\")\n",
    "        elif review != None:\n",
    "            review=review.get_text()\n",
    "            review_li.append(review[1:-1])\n",
    "# print(review_li)\n",
    "# print(len(review_li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining all rows of the table\n",
    "\n",
    "row_data=[]\n",
    "for t in range(1,pages+1):\n",
    "    \n",
    "    for rev in review_box:\n",
    "        table=rev.find(\"table\",{\"class\":\"review-ratings\"})\n",
    "        rows = table.find_all(\"tr\")\n",
    "        row_data.append(rows)\n",
    "#print(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "# moving all table keys and values into a dictionary\n",
    "\n",
    "for t in range(1,pages+1):\n",
    "\n",
    "    j=0\n",
    "    while j < 100:\n",
    "        ele_li=[]\n",
    "        \n",
    "        for ele in row_data[j]:\n",
    "            col = ele.find_all(\"td\")\n",
    "            ele_li.append(col)\n",
    "        \n",
    "        i=0\n",
    "\n",
    "\n",
    "        while i < len(ele_li):\n",
    "            \n",
    "\n",
    "\n",
    "            for k in columns:\n",
    "                dict2[k]\n",
    "                if ele_li[i][0].get_text() == k:\n",
    "                    dict_val=ele_li[i][1].get_text()\n",
    "                    \n",
    "\n",
    "                    dict2[k].append( dict_val)\n",
    "        \n",
    "            for k in valued_col:\n",
    "                if ele_li[i][0].get_text() == k:\n",
    "\n",
    "                    for q in ele_li[i]:\n",
    "                        rate = q.find_all(\"span\",{\"class\":\"star fill\"})\n",
    "                        str=\"\"\n",
    "                        for x in rate:\n",
    "                            str +=x.get_text()\n",
    "                    dict2[k].append(str[-1])\n",
    "\n",
    "\n",
    "            i +=1\n",
    "\n",
    "        lens = dict2.keys()\n",
    "        for item in lens:\n",
    "            if len(dict2[item]) != len(dict2[\"Recommended\"]):\n",
    "                dict2[item].append(\"NA\")\n",
    "            \n",
    "            \n",
    "        j+=1\n",
    "\n",
    "\n",
    "print(\"successful\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "lens = dict2.values()\n",
    "for item in lens:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={ \"Rating\":rating_li,\n",
    "      \"Review\":review_li,\n",
    "      \"Verification\":verify_li,\n",
    "      \"Customer\":name_li,\n",
    "      \"Published Date\":date_li}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating two dictionary\n",
    "\n",
    "dict={**dict1,**dict2}\n",
    "#dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape=pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complted\n"
     ]
    }
   ],
   "source": [
    "scrape.to_csv(\"reviews_BA.csv\",index=False)\n",
    "print(\"complted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
